---
title: "Analysis of Variance"
author: "GaÃ«lle Cordier"
date: "24/11/2014"
output: html_document
---

$\\$

**References**: <a name=refs></a>

* Main reference: The R Book - Chapter 11. Analysis of Variance

* Hyperlinks: links to wikipedia

* Other References:

    + [[1](http://onlinestatbook.com/2/analysis_of_variance/one-way.html)] One-Factor ANOVA (Between Subjects)

    + [2] Statistics: Principles and Methods (6th Edition): [ref1](#ref1)

---
```{r load_libraries,echo=FALSE,message=FALSE}
library(knitr)
library(reshape2)
library(doBy)
library(ggplot2)
library(gridExtra)
library(gtable)
```

```{r figures_path,echo=FALSE,message=FALSE}
opts_chunk$set(fig.path = "figures/") # Set figures path
```

ANOVA compares (two or more) means by comparing variances; one-Way ANOVA compares (two or more) means based on one factor by comparing variances.

Data example: crop yields per unit area measured from 10 randomly selected fields on each of 3 soil types (sand, clay, and loam) 

```{r data_yields}
yields<-data.frame(sand=c(6,10,8,6,14,17,9,11,7,11),
                   clay=c(17,15,3,11,14,12,12,8,10,13),
                   loam=c(13,16,9,12,15,16,17,13,18,14))
yields
```

We may want to know whether soil types (categorical explanatory variable or factor) significantly affects crop yield (numerical respone variable):

$H_0: \mu_s = \mu_c = \mu_l$

$H_1: \text{at least one mean is different}$

If we take a look at the sample means:

```{r reshape_data}
# reshape data to long format
yields_l<-melt(data = yields,
               value.name = "yield",
               measure.vars = c("sand","clay","loam"),
               variable.name = c("soil"))

head(yields_l); tail(yields_l)
```

```{r soil_means}
# means
means<-summaryBy(formula = yield~soil,data = yields_l,FUN = mean); means
# or:
aggregate(yield~soil, yields_l, mean)
```

and at the distribution of the yield values within the 3 soils:

```{r plot_soils}
ggplot(data = yields_l,aes(x = soil,y = yield, color = soil))+
    geom_boxplot()+
    stat_summary(fun.y=mean, geom="point", shape=4)+
    labs(x="")+
    scale_color_discrete(guide=F)
```

we can see that yield may turn out to be significantly different between sand and loam soils (their boxes don't overlap), but it is not as clear whether clay yield will be significantly greater/lower than sand/loam yield.

ANOVA allows us to make inferences about differences between means by looking at differences between variances.

### ANOVA assumptions:

---

* Normality of the response varirable (yield)

* Heterocedasticity: homogeneity of variances between groups (soil types)
$\\$

**Test for normality**

quantile-quantile plot (qqplot):

```{r qplot_normality}
with(yields_l,qqnorm(yield))
with(yields_l,qqline(yield,lty=2))
```


Shapiro-Wilk normality test:



**Test for homocedasticity**

### [Deviation (variability) and variance:](http://en.wikipedia.org/wiki/Partition_of_sums_of_squares)

---

The distance from any point in a collection of data, to the mean of the data (sample mean), is the deviation. This can be written as:

$y_i-\bar{y}$

where $y_i$ is the *ith* data point, and $\bar{y}$ is the estimate of the mean. If all such deviations are squared (so all differences are positives), and then summed, as in:

$\sum_{i=1}^n\left(y_i-\bar{y}\,\right)^2$

this gives the "sum of squares" for these data.

The deviation is unscaled (the sum of squares will grow with the size of the data collection), so to compare samples of different sizes we need to scale it by dividing it by the degrees of freedom (the number of parameters of the system that may vary independently, or to simplify, the sample size minus 1). In fact, the sample variance of a discrete random variable is defined as:

$S^2=\frac{\sum_{i=1}^{n}(y_i-\bar{y})^2}{n-1}$

So, the deviation is an unscaled measure of dispersion (or variability), that when scaled for the number of degrees of freedom estimates the variance.

### Partition of Sum of Squares:

---

The analysis of variance involves calculating the total variation or variability in the response variable (yield in this case) and partitioning it into two components: the intra-groupal or unexplained variability (variability in each group due to unknown factors) and the inter-groupal or explained variability (variability due to the explanatory variable, soil in this case). This is called the partition of sum of squares, and it allows us to quantify the relative importance of each one of said sources of variability: in our case, if the factor soil has an effect over crop yield, we would expect the total variability to be explained in greater measure by the explained variability than by the unexplained variability:

$\text{TOTAL VARIABILITY = EXPLAINED VARIABILITY + UNEXPLAINED VARIABILITY}$

### Calculating the Sums of Squares

---

#### **SSE**

We can define the variability in the response variable within each group as:

$\text{SSE}=\sum_{i=1}^{k}\sum_{i=j}^{n}(y_{ij}-\bar{y})^2$

that is, the sum of squares of the differences between the observation *j* (*n=10* replicates) within each group and the mean of said group *i* (*k=3* factor levels). This would be the unexplained variation or residual variability (error sum of squares) since it is not explained by the differences between groups.

```{r SSE}
# data with means
yields_l2<-data.frame(yields_l,mean=rep(means$yield.mean,each = 10))
yields_l2

SSE<-with(data = yields_l2,expr = sum((yield-mean)^2))

SSE
```

$\\$

#### **SSA**

In a similar way, we can define the variability in the response variable between groups as:

$\text{SSA}=\sum_{i=1}^{k}\sum_{i=j}^{n}(\bar{y_{i}}-\bar{\bar{y}})^2=n*\sum_{i=1}^{k}(\bar{y_{i}}-\bar{\bar{y}})^2$

that is, the sum of squares of the differences between the individual treatment means *n x i* and the overall mean (the mean of all observations

```{r ov.mean1}
ov.mean<-with(yields_l2, mean(yield)); ov.mean
```

or the mean of the group means).

```{r ov.mean2}
ov.mean<-with(means, mean(yield.mean)); ov.mean
```

This would be the explained variability (the treatment sum of squares).

$\text{SSA}=n*\sum_{i=1}^{k}(\bar{y_{i}}-\bar{\bar{y}})^2$

```{r SSA_1}
SSA<-with(means, 10*sum((yield.mean-ov.mean)^2)); SSA
```

$\text{SSA}=\sum_{i=1}^{k}\sum_{i=j}^{n}(\bar{y_{i}}-\bar{\bar{y}})^2$

```{r SSA_2}
with(yields_l2, sum((mean-ov.mean)^2))
```

$\\$

#### **SST**

The total variability (the total sum of squares) would be then:

$\text{SST}=\sum_{i=1}^{k}\sum_{i=j}^{n}(y_{ij}-\bar{\bar{y}})^2$

that is, the sum of squares of the differences between the observation *ij* and the overall mean.

```{r SST}
SST<-with(data = yields_l2,expr = sum((yield-ov.mean)^2))

SST
```

$\\$

**Plotting the variability**

```{r SSE_plots}
plotdata<-data.frame(yields_l2,
                     ov.mean, # no need to rep()
                     x=seq(from = 0,to = 30,length.out = nrow(yields_l2)))

p1<-ggplot(data = plotdata,aes(x=x,y=yield,shape=soil,color=soil))+
    geom_point(size=2)+
    labs(x="",y="",title="Observations")
    # if we want to make the legend horizontal:
#     guides(color=guide_legend(direction="horizontal",title.position="top",title.hjust=0.5),
#            shape=guide_legend(direction="horizontal",title.position="top",title.hjust=0.5))

p2<-ggplot(data = plotdata,aes(x=x,y=yield,shape=soil,color=soil))+
    geom_point(size=2)+
    geom_line(aes(y=mean))+
    geom_linerange(aes(ymin=yield,ymax=mean))+
    labs(x="",y="",title="SSE")

p3<-ggplot(data = plotdata,aes(x=x,y=yield,shape=soil,color=soil))+
    geom_point(size=2)+
    geom_line(aes(y=mean),size=1)+
    geom_abline(intercept=11.9,slope=0)+
    geom_linerange(aes(ymin=mean,ymax=ov.mean))+
    labs(x="",y="",title="SSA")

p4<-ggplot(data = plotdata,aes(x=x,y=yield,shape=soil,color=soil))+
    geom_point(size=2)+
    geom_abline(intercept=11.9,slope=0)+
    geom_linerange(aes(ymin=yield,ymax=ov.mean))+
    labs(x="",y="",title="SST")

legend<-gtable_filter(ggplot_gtable(ggplot_build(p1)), "guide-box") # extract legend from p1

grid.arrange(arrangeGrob(p1 + theme(legend.position="none"), 
                         p2 + theme(legend.position="none"),
                         p3 + theme(legend.position="none"),
                         p4 + theme(legend.position="none"), 
                         nrow = 2,
                         main = textGrob("Sums of Squares", vjust = 1, gp = gpar(fontsize=15)),
                         left = textGrob("Yield", rot = 90, vjust = 1)),
             legend,ncol=2,widths=c(4/5,1/5))
```

*(extraction of legend and grid.arrange with global Y-axis and common legend as seen [here](http://stackoverflow.com/questions/11076567/plot-a-legend-and-well-spaced-universal-y-axis-and-main-titles-in-grid-arrange))*

$\\$

**In conclusion**

The total sum of squares is the sum of the treatment sum of squares and the error sum of squares:

$SST = SSA + SSE$

```{r check_SST}
all.equal(SST,SSA+SSE)
```

So the difference between SST and SSE is the treatment sum of squares, SSA:

$SSA = SST - SSE$

This is the amount of the variation in yield that is explained by the differences between the treatment means.

### Drawing the ANOVA table: the F-test

---

**Source of variation**

- Explained variability: type of soil
- Unexplained variability: error
- Total variability

```{r var_source}
Source<-c("Soil type","Error","Total")
```

$\\$

**Sum of squares**

- SSA
- SSE
- SST

```{r SS}
SS<-c(SSA,SSE,SST)
```

$\\$

[**Degrees of freedom**](http://en.wikipedia.org/wiki/Degrees_of_freedom_%28statistics%29)

The degrees of freedom are the number of parameters that may vary independently (the number of observations minus 1).

- Treatment degrees of freedom: we are comparing 1 mean per soil type with the overall mean (3 parameters), so we have:   
$3-1=2$ degrees of freedom

- Error degrees of freedom: we are comparing 10 observations per soil type with the soil type mean (10 parameters by soil type), so we have:   
$(10-1)*3=9*3=27$ degrees of freedom

- Total degrees of freedom: we are comparing 30 observations with the overall mean (30 parameters), so we have:   
$30-1=29$ degrees of freedom

```{r df}
df<-c(2,27,29)
```

$\\$

**Mean square**

The mean square is obtained by dividing the sum of squares by the degrees of freedom, and is a measure of the treatment variance and the error variance:

```{r MS}
MS<-SS/df
```

The treatment variance is the mean square between groups ($MS_B$):

```{r MS_B}
MS_B<-MS[1]
MS_B
```

If the group means are equal ($H_0$ is true), $MS_B$ is an estimator of the population variance $\sigma^2$ [[1](http://onlinestatbook.com/2/analysis_of_variance/one-way.html)].

The error variance or the mean square error ($MSE$) is the variance within groups (also $MS_W$), and since there is equal replication in each soil type, it is equal to the mean of the variances of the soil types:

```{r MS_W}
MS_W<-MS[2]
MS_W
vars<-aggregate(yield~soil, yields_l, var); vars
mean(vars$yield)
```

Since we assume homogeneity of variances, it is also an estimator of the population variance [[1](http://onlinestatbook.com/2/analysis_of_variance/one-way.html)], whether the population means are equal or not. This estimator is also called the *pooled variance* because it is calculated across all the treatments.

The total variance is the total mean square, and it is equal to the variance of all the observations with respect to the overall mean:

```{r}
MS_T<-MS[3]
MS_T
with(yields_l2,sum((yield-ov.mean)^2)/29)
# or
var(yields_l2$yield)
```

$\\$

**F ratio**

$F=\frac{MS_B}{MS_W}$

The *F* ratio tests the null hypothesis that the treatment means are all the same:

$H_0: \mu_s = \mu_c = \mu_l$

$H_1: \text{at least one mean is significantly different from the others}$

If the null hypothesis isn't true, we would expect the variability between groups $MS_B$ to be greater than the variability within groups $MS_W$, so we would expect the F-ratio to be > 1. On the contrary, if the null hypothesis is true, we expect the F-ratio to have a value close to 1. 

In terms of variance estimation [[1](http://onlinestatbook.com/2/analysis_of_variance/one-way.html)]:

* If the population means are equal, then both $MSE$ and $MS_B$ are estimates of $\sigma^2$ and should therefore be about the same (they will not be exactly the same since they are just estimates and are based on different aspects of the data: the $MS_B$ is computed from the sample means and the $MSE$ is computed from the sample variances). 

* If the population means are not equal, then $MSE$ will still estimate $\sigma^2$ because differences in population means do not affect variances. However, differences in population means affect $MSE$ since differences among population means are associated with differences among sample means. So, $MSE$ estimates $\sigma^2$ whether or not the population means are equal, but $MS_B$ doesn't if they are not.

Therefore, if the $MS_B$ is much larger than the $MSE$, then the population means are unlikely to be equal, but if the $MS_B$ is about the same as $MSE$, then the data are consistent with the null hypothesis that the population means are equal. 

```{r F_ratio}
FR<-MS_B/MS_W
FR
```

Is this value significant (is $MS_B$ significantly greater than $MS_W$)? To answer this we must compare the test statistic $F=4.24$ with the critical value of $F$, that is, the value in the distribution from which we would be rejecting the null hypothesis. This critical value is the quantile of the probability distribution for a given probability ($0.95$ if $\alpha=0.05$) and degrees of freedom ($df=2$ for the numerator and $df=27$ for the denominator):

$Q(p)\,=\,\inf\left\{ x\in \mathbb{R} : p \le F(x) \right\}$

```{r qf}
qf(p = 0.95,df1 = 2,df2 = 27)
```

that is, the value $x$ in the $F(x)$ distribution for which the cumulative probability $p = 0.95$ (left tail of the distribution).

As the F-test > critical value, we would reject the null hypothesis. But, in order to be able to work independently from the confidence interval, we use the function for cumulative probabilities of the $F$ distribution instead of using the function for quantiles, so what we are looking for now is the probability of being equal or greater than our quantile (right tail of the distribution); that would be the inverse of the cumulative distribution:

$P(x \geq Ft) = 1 - P(x \leq Ft)$

```{r pf}
p_value<-1-pf(q = FR,df1 = 2,df2 = 27)
p_value
#or
pf(q = FR,df1 = 2,df2 = 27,lower.tail = F)
```

This is the **p-value**, that is, the probability for a value of being equal or greater than 4.24 when the null hypothesys is true; in this case, the probability of a value being equal or greater than 4.24 by chance would be 0.025 (25 times in 1000). If we are rejecting at a confidence level of 5% ($\alpha=0.05$), we would reject the null hypothesis since $p-value < \alpha$.

$\\$

**The ANOVA table**

```{r AOV_table}
AOV.table<-data.frame(Source,SS,df,MS,
                      FR=c(round(FR,3),"",""),
                      p_value=c(round(p_value,3),"",""))
kable(x = AOV.table,digits = 3)
```

This can be done fitting our analysis-of-variance model with `aov`, and then calling the `summary`:

```{r summary_aov}
yields.aov<-with(yields_l,aov(formula = yield~soil))
summary(yields.aov)
```

$\\$

**In conclusion**

We reject the $H_0: \mu_s = \mu_c = \mu_l \rightarrow$ there are significant differences between soil types (at least one mean is different).

### The T-test

The same ANOVA table can be drawn with the `anova` function after fitting the model with `lm`:

```{r anova_lm}
yields.lm<-with(yields_l,lm(formula = yield~soil))
anova(yields.lm)
```

If we do the summary of the `lm` fit, we have:

```{r summary_lm}
sum.lm<-summary(yields.lm)
sum.lm$coefficients
```

The main difference between the `aov` and the `lm` approach is the way `summary` handles the fit: 

* `summary(fit.aov)` expresses the fit in terms of analysis of variance
* `summary(fit.lm)` expresses the fit in terms of linear models (the intercept, the slope, and their standard errors)

In the context of aov, the *Intercept* is a mean value; in this case, it corresponds to the mean of the first factor level (*soilsand*). The slopes are differences between means; each one corresponds to the difference between the means of each other factor (*soilclay* and *soilloam*) and the mean of the first one on the Intercept.

$\\$

**Estimate**

The estimate for Intercept is the mean of *soilsand*, the estimate for *soilclay* is the difference between the means of *soilclay* and *soilsand*, and the estimate for *soilloam* is the difference between the means of *soiloam* and *soilsand*:

```{r estimates}
sum.lm$coefficients[,1]

# means
means
# sand mean
means$yield.mean[1]
# clay mean - sand mean
means$yield.mean[2]-means$yield.mean[1]
# loam mean - sand mean
means$yield.mean[3]-means$yield.mean[1]
```

$\\$

[**Standard error**](http://en.wikipedia.org/wiki/Standard_error)

The standard deviation of a population, $\sigma$ (the square root of the variance population, $\sigma^2$), quantifies the deviation of individuals with respect to the true (the population) mean, $\mu$:

$\sigma = \sqrt{\sigma^2} = \sqrt{\frac{\sum_{i=1}^n (y_i - \mu)^2}{n}}, {\rm \ \ where\ \ } \mu = \frac{\sum_{i=1}^n y_i}{n}$

Its estimator $S$, the standard deviation of a sample, quantifies the deviation of individuals with respect to the sample mean, $\bar{y}$:

$S = \sqrt{S^2} = \sqrt{\frac{\sum_{i=1}^{n}(y_i-\bar{y})^2}{n-1}}$

The standard deviation of a point estimator is also called its standard error, so the standard error of the mean, $SE_\text{mean}$, quantifies the deviation of a sample mean $\bar{y}$ with sample size = *n*, with respect to the true mean $\mu$:

<a name=ref1></a>

[[2](#refs)] p. 299-301 

$E(\bar{y})=\mu$

$S(\bar{y})=\frac{\sigma}{\sqrt{n}} \rightarrow SE(\bar{y})=\frac{\sigma}{\sqrt{n}} \rightarrow \text{Estimated SE}(\bar{y})=\sqrt{\frac{S^2}{n}}$

Since the Intercept is a mean, the standard error associated is the **standard error of the mean**, where the variance $S^2$ here is the [common variance](http://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test) or [pooled variance](http://en.wikipedia.org/wiki/Pooled_variance) since we are considering that the variability comes from the existing variability within groups $\rightarrow$ this pooled variance is the error variance $MSE$, or variance within groups $MS_W$ (remember, the mean of the variances of the soil types under the assumption of heterocedasticity):

$S_{y_{1}y_{2}}=\sqrt{\frac{1}{2}(S_{y_{1}}^2+S_{y_{2}}^2)}$

So the standard error of the mean is:

$SE_\text{mean} = \sqrt{\frac{MS_W}{n}}$

where *n* is the sample size of each group.

```{r SE_mean}
SE_mean<-sqrt(MS_W/10)
SE_mean
sum.lm$coefficients[,2]
```

It is a measure of how accurate our estimate of the mean is likely to be, given the existing variability within groups, and it is equal for each one of the soil type means since the number of replicates is equal.

The standard error of the difference between two means, $SE_\text{diff}$, quantifies the existing deviation in the difference between sample means:

$SE_\text{diff}=S_{y_{1}y_{2}} \sqrt{\frac{2}{n}}$

where the combined standard deviation:

$S_{y_{1}y_{2}}=\sqrt{\frac{1}{2}(S_{y_{1}}^2+S_{y_{2}}^2)}$

```{r}
sum.lm$coefficients
```

As the second and third row of the `lm.fit` summary are differences between means, the standard error associated is the **standard error of the difference between two means**, where the variance is again the error variance $S_{y_{1}}^2=S_{y_{2}}^2=MS_W$, and *n* is the number of observations in each group:

$S_{y_{1}y_{2}}=\sqrt{\frac{1}{2}(2*MS_W)}$

$SE_\text{diff}=\sqrt{\frac{1}{2}(2*MS_W)} \sqrt{\frac{2}{n}} = \sqrt{2*\frac{MS_W}{n}}$

```{r SE_diff}
SE_diff<-sqrt(2*(MS_W/10))
SE_diff
sum.lm$coefficients[,2]
```

This is the standard error we need for doing a *t* test to compare any two means:

$t = \frac{\text{a difference}}{\text{standard error of the difference}} \rightarrow t = \frac{\bar{y_{1}}-\bar{y_{2}}}{SE_{diff}}$ $(gl=2*(n-1)=2n-2)$

$\\$

**t-value**

So, the t-value for each pair of soil means would be:

$t_{clay-sand} = \frac{\bar{y}_{clay}-\bar{y}_{sand}}{SE_\text{diff}} = \frac{\text{Estimate}_{c}}{\text{Std. Error}}$

```{r t_value_cs}
Estimate<-as.numeric(sum.lm$coefficients[,1])
Std.Error<-as.numeric(sum.lm$coefficients[,2])
t_value<-as.numeric(sum.lm$coefficients[,3])

t_cs<-Estimate[2]/Std.Error[2]; t_cs
# or
with(means,yield.mean[2]-yield.mean[1])/SE_diff

t_value[2]
```

$t_{loam-sand} = \frac{\bar{y}_{loam}-\bar{y}_{sand}}{SE_\text{diff}} = \frac{\text{Estimate}_{l}}{\text{Std. Error}}$

```{r t_value_ls}
t_ls<-Estimate[3]/Std.Error[3]; t_ls
# or
with(means,yield.mean[3]-yield.mean[1])/SE_diff

t_value[3]
```

Note that $t_{loam-clay}$ does not appear in the table: 

```{r}
sum.lm$coefficients
```

We have to calculate it by substracting their estimates (which equals substracting their means) since the differences on the summary table are all with respect to the Intercept:

$t_{loam-clay} = \frac{\bar{y}_{loam}-\bar{y}_{clay}}{SE_\text{diff}} = \frac{\text{Estimate}_{l}-\text{Estimate}_{c}}{\text{Std. Error}}$

```{r t_value}
t_lc<-(Estimate[3]-Estimate[2])/SE_diff; t_lc
# or
with(means,yield.mean[3]-yield.mean[2])/SE_diff
```

In order to know if the estimates of the differences between means with their associated standard error are significant, we look for the critical value for the $t$ distribution with $df=18$ and $p=1-\alpha/2=1-0.05/2=0.975$ (it's a two tailed test!):

```{r t_CV}
t_CV<-qt(0.975, 18); t_CV
# or:
qt(0.025,18,lower.tail = F)
```

and we see if our *t* is greater than the critical value (if we reject the $H_0: \mu_1 = \mu_2$):

```{r}
t_cs>t_CV
t_ls>t_CV
t_lc>t_CV
```

$\rightarrow$ we would reject: $H_0: \mu_{\text{sand}} = \mu_{\text{loam}}$

Or better, we look for the **p-value** associated with our *t* in the $t$ distribution (again, it's a two-tailed test so we multiply it by two):

```{r p_value}
pv_cs<-2*(1-pt(t_cs,18)); pv_cs
# or:
2*pt(t_cs,18,lower.tail = F)

pv_ls<-2*(1-pt(t_ls,18)); pv_ls
# or:
2*pt(t_ls,18,lower.tail = F)

pv_lc<-2*(1-pt(t_lc,18)); pv_lc
# or:
2*pt(t_lc,18,lower.tail = F)
```

and we see if the p-value is smaller than the confidence level $\alpha$ (if we reject the $H_0$):

```{r}
pv_cs<0.05 # not exactly equal in the table
pv_ls<0.05 # not exactly equal in the table (pv_ls<0.01)
pv_lc<0.05
```

$\rightarrow$ again we would reject: $H_0: \mu_{\text{sand}} = \mu_{\text{loam}}$

We can see this graphically with a bar-plot:

```{r barplots}

```


So why use ANOVA instead of t-test? $\rightarrow$ ANOVA can test more than one treatment (without taking more risk of making an error type I by doing a t-test for each pair of means), and also allows us to quantify the variability due to them.